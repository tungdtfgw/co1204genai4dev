<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Slide 18 — PEFT Explained: LoRA and QLoRA</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="slide fade-in">
    <div class="slide-header"></div>
    <div class="slide-content">
      <h1 class="slide-title">PEFT Explained: LoRA and QLoRA</h1>
      <div class="two-panel">
        <div class="panel slide-in-left">
          <img src="images/lora_diagram.png" alt="LoRA diagram" class="panel-image">
        </div>
        <div class="panel panel-text slide-in-right">
          <ul class="content-list template3-list">
            <li><strong>LoRA (Low-Rank Adaptation)</strong><br>Instead of fine-tuning all weights, LoRA fine-tunes two much smaller matrices that approximate the weight changes.<br>This results in a small "LoRA adapter" (measured in MBs) that can be applied to the original, frozen LLM.</li>
            <li><strong>QLoRA (Quantized Low-Rank Adaptation)</strong><br>An even more memory-efficient version of LoRA.<br>QLoRA quantizes the LLM's weights to a lower precision (e.g., 4-bit) before adding the LoRA adapters.<br>This allows fine-tuning of large models on consumer-grade GPUs.</li>
          </ul>
        </div>
      </div>
    </div>
    <div class="slide-footer">
      <div class="footer-nav"><button class="nav-btn" onclick="previousSlide()">◀ Prev</button></div>
      <div class="footer-center"><div class="footer-text">All these resources belong to FPT Greenwich. Any unauthorized copying, alteration, distribution outside our organization is strictly prohibited</div></div>
      <div class="footer-nav"><button class="nav-btn" onclick="nextSlide()">Next ▶</button></div>
    </div>
  </div>
  <script src="presentation.js"></script>
</body>
</html>
