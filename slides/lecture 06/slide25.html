<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Slide 25 — The Future is Multimodal</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="slide fade-in">
    <div class="slide-header"></div>
    <div class="slide-content">
      <h1 class="slide-title">The Future is Multimodal</h1>
      <ul class="content-list template1-list wide">
        <li class="slide-in-center">The next frontier for LLMs is multimodality—processing information from text, images, video, and audio.</li>
        <li class="slide-in-center" style="animation-delay:0.2s">Vision-Language Models (VLMs) combine visual and text information.</li>
        <li class="slide-in-center" style="animation-delay:0.4s">Audio or Speech LLMs can understand and generate language from audio inputs (e.g., OpenAI's Whisper).</li>
        <li class="slide-in-center" style="animation-delay:0.6s">PEFT techniques like LoRA and QLoRA are already being successfully applied to fine-tune these powerful multimodal models for specialized tasks.</li>
      </ul>
    </div>
    <div class="slide-footer">
      <div class="footer-nav"><button class="nav-btn" onclick="previousSlide()">◀ Prev</button></div>
      <div class="footer-center"><div class="footer-text">All these resources belong to FPT Greenwich. Any unauthorized copying, alteration, distribution outside our organization is strictly prohibited</div></div>
      <div class="footer-nav"><button class="nav-btn" onclick="nextSlide()">Next ▶</button></div>
    </div>
  </div>
  <script src="presentation.js"></script>
</body>
</html>
